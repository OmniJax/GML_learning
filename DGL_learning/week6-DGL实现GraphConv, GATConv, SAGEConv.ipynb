{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqM5Z7qKUgqS"
   },
   "source": [
    "In this week, you are required to implement a toy GATConv and SAGEConv based on document. Also, you need to implement both in PyG and DGL. In this work, you will get a further understanding of tensor-centric in PyG and graph-centric in DGL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphConv\n",
    "Mathematically it is defined as follows:\n",
    "\n",
    "$$\n",
    "  h_i^{(l+1)} = \\sigma(b^{(l)} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ji}}h_j^{(l)}W^{(l)})\n",
    "$$\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of node $i$, \n",
    "$c_{ji}$ is the product of the square root of node degrees\n",
    "$(i.e.,  c_{ji} = \\sqrt{|\\mathcal{N}(j)|}\\sqrt{|\\mathcal{N}(i)|})$,\n",
    "and $\\sigma$ is an activation function.\n",
    "\n",
    "If a weight tensor on each edge is provided, the weighted graph convolution is defined as:\n",
    "\n",
    "$$\n",
    "  h_i^{(l+1)} = \\sigma(b^{(l)} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{e_{ji}}{c_{ji}}h_j^{(l)}W^{(l)})\n",
    "$$\n",
    "where $e_{ji}$is the scalar weight on the edge from node $j$ to node $i$.\n",
    "This is NOT equivalent to the weighted graph convolutional network formulation in the paper.\n",
    "\n",
    "To customize the normalization term :$c_{ji}$, one can first set ``norm='none'`` for\n",
    "the model, and send the pre-normalized :$e_{ji}$ to the forward computation. We provide\n",
    ":class:`~dgl.nn.pytorch.EdgeWeightNorm` to normalize scalar edge weight following the GCN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGL_GraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DGL_GraphConv, self).__init__()\n",
    "        self.W = nn.Parameter(torch.rand(in_channels, out_channels))\n",
    "        self.b = nn.Parameter(torch.rand(out_channels))\n",
    "        self.activate = nn.ReLU()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            # 这里的normalization用了left+right，也就是考虑了出度+入度\n",
    "            norm_src = g.out_degrees().clamp(min=1).view(-1, 1)\n",
    "            norm_src = torch.pow(norm_src, -0.5)\n",
    "\n",
    "            feat_src = norm_src * h\n",
    "            feat_src = torch.matmul(feat_src, self.W)\n",
    "            g.srcdata[\"h\"] = feat_src\n",
    "            g.update_all(fn.u_mul_e(\"h\", \"edge_weight\", \"he\"), fn.sum(\"he\", \"rst\"))\n",
    "            rst = g.dstdata[\"rst\"]\n",
    "\n",
    "            norm_dst = g.in_degrees().clamp(min=1).view(-1, 1)\n",
    "            norm_dst = torch.pow(norm_dst, -0.5)\n",
    "            rst = rst * norm_dst\n",
    "\n",
    "            rst += self.b\n",
    "            return F.relu(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=5, num_edges=6,\n",
       "      ndata_schemes={'h': Scheme(shape=(8,), dtype=torch.float32)}\n",
       "      edata_schemes={'edge_weight': Scheme(shape=(), dtype=torch.float32)})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.tensor([0, 1, 1, 2, 2, 4])\n",
    "dst = torch.tensor([2, 0, 2, 3, 4, 3])\n",
    "h = torch.ones((5, 8))\n",
    "\n",
    "g = dgl.graph((src, dst))\n",
    "g.ndata[\"h\"] = h\n",
    "edge_weight = torch.ones(g.num_edges())  # 给各个边赋格权重\n",
    "g.edata[\"edge_weight\"] = edge_weight\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = h.shape[1]\n",
    "out_channels = in_channels  # *2\n",
    "dgl_graphConv = DGL_GraphConv(in_channels, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1296, 4.5994, 3.6825, 3.6156, 2.8758, 3.3465, 2.9255, 4.2599],\n",
       "        [0.2342, 0.9638, 0.6446, 0.8826, 0.4221, 0.4741, 0.7989, 0.4236],\n",
       "        [3.4698, 7.1701, 5.8306, 5.5481, 4.6109, 5.3777, 4.4293, 6.9727],\n",
       "        [3.4698, 7.1701, 5.8306, 5.5481, 4.6109, 5.3777, 4.4293, 6.9727],\n",
       "        [2.1296, 4.5994, 3.6825, 3.6156, 2.8758, 3.3465, 2.9255, 4.2599]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl_graphConv(g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATConv\n",
    "Graph attention layer from Graph Attention Network\n",
    "$$h_i^{(l+1)} = \\sum_{j\\in \\mathcal{N}(i)} \\alpha_{i,j} W^{(l)} h_j^{(l)}$$\n",
    "\n",
    "where $\\alpha_{ij}$ is the attention score bewteen node $i$ and\n",
    "node $j$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\\begin{aligned}\\alpha_{ij}^{l} &= \\mathrm{softmax_i} (e_{ij}^{l})\\\\e_{ij}^{l} &= \\mathrm{LeakyReLU}\\left(\\vec{a}^T [W h_{i} \\| W h_{j}]\\right)\\end{aligned}\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGL_GATConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DGL_GATConv,self).__init__()\n",
    "        self.W=nn.Parameter(torch.rand(in_channels,out_channels))\n",
    "        self.a=nn.Parameter(torch.rand(2*in_channels,1))\n",
    "        self.leakyrelu=nn.LeakyReLU()\n",
    "        pass\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # 参考了源码，论文中先将[Wh_i||Wh_j]拼接，再计算a[Wh_i||Wh_j]，\n",
    "        # 这样会使[Wh_i||Wh_j]更大的矩阵再边上传输\n",
    "        # 因此先将a分解为[a_i||a_j]，然后进行a^T@[Wh_i || Wh_j] = a_l@Wh_i + a_r@Wh_j\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=5, num_edges=6,\n",
       "      ndata_schemes={'h': Scheme(shape=(8,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.tensor([0, 1, 1, 2, 2, 4])\n",
    "dst = torch.tensor([2, 0, 2, 3, 4, 3])\n",
    "h = torch.ones((5, 8))\n",
    "\n",
    "g = dgl.graph((src, dst))\n",
    "g.ndata[\"h\"] = h\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels=8\n",
    "out_channels=4\n",
    "h_input=g.ndata['h']\n",
    "W=torch.ones(in_channels,out_channels)\n",
    "a=torch.ones(2*in_channels,1)\n",
    "\n",
    "with g.local_scope():\n",
    "    hW=h_input@W    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=torch.tensor([\n",
    "    [1,11,111,1111],\n",
    "    [2,22,222,2222],\n",
    "    [3,33,333,3333]\n",
    "],dtype=torch.float)\n",
    "a=torch.ones(4*2,1,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.1000e+01, 1.1100e+02, 1.1110e+03],\n",
       "        [1.0000e+00, 1.1000e+01, 1.1100e+02, 1.1110e+03],\n",
       "        [1.0000e+00, 1.1000e+01, 1.1100e+02, 1.1110e+03],\n",
       "        [2.0000e+00, 2.2000e+01, 2.2200e+02, 2.2220e+03],\n",
       "        [2.0000e+00, 2.2000e+01, 2.2200e+02, 2.2220e+03],\n",
       "        [2.0000e+00, 2.2000e+01, 2.2200e+02, 2.2220e+03],\n",
       "        [3.0000e+00, 3.3000e+01, 3.3300e+02, 3.3330e+03],\n",
       "        [3.0000e+00, 3.3000e+01, 3.3300e+02, 3.3330e+03],\n",
       "        [3.0000e+00, 3.3000e+01, 3.3300e+02, 3.3330e+03]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl=h.repeat(1,N).view(N*N,-1)\n",
    "hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.1000e+01, 1.1100e+02, 1.1110e+03],\n",
       "        [2.0000e+00, 2.2000e+01, 2.2200e+02, 2.2220e+03],\n",
       "        [3.0000e+00, 3.3000e+01, 3.3300e+02, 3.3330e+03],\n",
       "        [1.0000e+00, 1.1000e+01, 1.1100e+02, 1.1110e+03],\n",
       "        [2.0000e+00, 2.2000e+01, 2.2200e+02, 2.2220e+03],\n",
       "        [3.0000e+00, 3.3000e+01, 3.3300e+02, 3.3330e+03],\n",
       "        [1.0000e+00, 1.1000e+01, 1.1100e+02, 1.1110e+03],\n",
       "        [2.0000e+00, 2.2000e+01, 2.2200e+02, 2.2220e+03],\n",
       "        [3.0000e+00, 3.3000e+01, 3.3300e+02, 3.3330e+03]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr=h.repeat(N,1)\n",
    "hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 8])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_cat=torch.concat([hl,hr],dim=1)\n",
    "h_cat.view(N,-1,in_channels).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=F.leaky_relu(h_cat@a)\n",
    "alpha=F.softmax(e,dim=0)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h_cat@a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4764, -2.1203]],\n",
       "\n",
       "        [[ 1.4764, -2.1203]],\n",
       "\n",
       "        [[ 1.4764, -2.1203]],\n",
       "\n",
       "        [[ 1.4764, -2.1203]],\n",
       "\n",
       "        [[ 1.4764, -2.1203]],\n",
       "\n",
       "        [[ 1.4764, -2.1203]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 1: Homogeneous graph\n",
    "from dgl.nn import GATConv\n",
    "g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))\n",
    "g = dgl.add_self_loop(g)\n",
    "feat = torch.ones(6, 10)\n",
    "gatconv = GATConv(10, 2, num_heads=1)\n",
    "res = gatconv(g, feat)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGEConv\n",
    "$$\n",
    "\\begin{align}\\begin{aligned}h_{\\mathcal{N}(i)}^{(l+1)} &= \\mathrm{aggregate}\n",
    "\\left(\\{h_{j}^{l}, \\forall j \\in \\mathcal{N}(i) \\}\\right)\\\\h_{i}^{(l+1)} &= \\sigma \\left(W \\cdot \\mathrm{concat}\n",
    "(h_{i}^{l}, h_{\\mathcal{N}(i)}^{l+1}) \\right)\\\\h_{i}^{(l+1)} &= \\mathrm{norm}(h_{i}^{(l+1)})\\end{aligned}\\end{align}\n",
    "$$\n",
    "If a weight tensor on each edge is provided, the aggregation becomes:\n",
    "$$\n",
    "h_{\\mathcal{N}(i)}^{(l+1)} = \\mathrm{aggregate}\n",
    "\\left(\\{e_{ji} h_{j}^{l}, \\forall j \\in \\mathcal{N}(i) \\}\\right)\n",
    "$$\n",
    "where $e_{ji}$ is the scalar weight on the edge from node $j$ to node $i$.\n",
    "    Please make sure that $e_{ji}$ is broadcastable with $h_j^{l}$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbdLhvBSVYEr"
   },
   "outputs": [],
   "source": [
    "class DGL_SAGEConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        pass\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G955HzNxVjSu"
   },
   "source": [
    "If you want to check your answer, you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKgfXLyLVwus"
   },
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 4], [2, 0, 2, 3, 4, 3]])\n",
    "x = torch.ones((5, 8))\n",
    "conv = PyG_GATConv(8, 4)\n",
    "output = conv(x, edge_index)\n",
    "print(output)\n",
    "conv = PyG_SAGEConv(8, 4)\n",
    "output = conv(x, edge_index)\n",
    "print(output)\n",
    "\n",
    "src = torch.tensor([0, 1, 1, 2, 2, 4])\n",
    "dst = torch.tensor([2, 0, 2, 3, 4, 3])\n",
    "h = torch.ones((5, 8))\n",
    "g = dgl.graph((src, dst))\n",
    "conv = DGL_GATConv(8, 4)\n",
    "output = conv(g, h)\n",
    "print(output)\n",
    "conv = DGL_SAGEConv(8, 4)\n",
    "output = conv(g, h)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GML",
   "language": "python",
   "name": "gml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
