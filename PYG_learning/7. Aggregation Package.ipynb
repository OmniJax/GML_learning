{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1358abdb-6530-4b8e-9358-bfb544041c20",
   "metadata": {},
   "source": [
    "# Customizing Aggregations within Message Passing\n",
    "\n",
    "we provide **modular and re-usable aggregations** in the newly defined `torch_geometric.nn.aggr.*` package. Unifying these concepts also helps us to perform optimization and specialized implementations in a single place. In the new integration, the following functionality is applicable:\n",
    "\n",
    "```python\n",
    "# Original interface with string type as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=\"mean\")\n",
    "\n",
    "\n",
    "# Use a single aggregation module as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=MeanAggregation())\n",
    "\n",
    "\n",
    "# Use a list of aggregation strings as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=['mean', 'max', 'sum', 'std', 'var'])\n",
    "\n",
    "\n",
    "# Use a list of aggregation modules as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=[\n",
    "            MeanAggregation(),\n",
    "            MaxAggregation(),\n",
    "            SumAggregation(),\n",
    "            StdAggregation(),\n",
    "            VarAggregation(),\n",
    "        ])\n",
    "\n",
    "\n",
    "# Use a list of mixed modules and strings as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=[\n",
    "            'mean',\n",
    "            MaxAggregation(),\n",
    "            'sum',\n",
    "            StdAggregation(),\n",
    "            'var',\n",
    "        ])\n",
    "\n",
    "\n",
    "# Define multiple aggregations with `MultiAggregation` module\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=MultiAggregation([\n",
    "            SoftmaxAggregation(t=0.1, learn=True),\n",
    "            SoftmaxAggregation(t=1, learn=True),\n",
    "            SoftmaxAggregation(t=10, learn=True)\n",
    "        ]))\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dea328-7a83-4e15-aed1-a7de730912cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "os.environ[\"TORCH\"] = torch.__version__\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1797cf-e9a3-4430-ad92-cad4d8489ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PubMed():\n",
      "==================\n",
      "Number of graphs: 1\n",
      "Number of features: 500\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root=\"data/Planetoid\", name=\"PubMed\", transform=NormalizeFeatures())\n",
    "print(f\"Dataset: {dataset}:\")\n",
    "print(\"==================\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7e9c5a-409b-44fe-8510-973862fc84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]  # Get the first graph object.\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3316549-98de-491e-bea4-d6f4655a8108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "train_loader = ClusterLoader(\n",
    "    cluster_data, batch_size=32, shuffle=True\n",
    ")  # 2. Stochastic partioning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7eea5f-282e-40f5-871f-69acf58ccab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    for sub_data in train_loader:  # Iterate over each mini-batch.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(\n",
    "            out[sub_data.train_mask], sub_data.y[sub_data.train_mask]\n",
    "        )  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "        accs.append(\n",
    "            int(correct.sum()) / int(mask.sum())\n",
    "        )  # Derive ratio of correct predictions.\n",
    "    return accs\n",
    "\n",
    "\n",
    "def run(model, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model)\n",
    "        train_acc, val_acc, test_acc = test(model)\n",
    "        print(\n",
    "            f\"Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca863ac0-ab20-4f11-9335-59e20d36ea0a",
   "metadata": {},
   "source": [
    "## Define a GNN class and Import Aggregations\n",
    "Now, let's define a GNN helper class and import all those new aggregation operators!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4a8511-36af-43fc-8a6b-2ec5462c1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    Aggregation,\n",
    "    MaxAggregation,\n",
    "    MeanAggregation,\n",
    "    MultiAggregation,\n",
    "    SAGEConv,\n",
    "    SoftmaxAggregation,\n",
    "    StdAggregation,\n",
    "    SumAggregation,\n",
    "    VarAggregation,\n",
    ")\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, aggr=\"mean\", aggr_kwargs=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(\n",
    "            dataset.num_node_features,\n",
    "            hidden_channels,\n",
    "            aggr=aggr,\n",
    "            aggr_kwargs=aggr_kwargs,\n",
    "        )\n",
    "        self.conv2 = SAGEConv(\n",
    "            hidden_channels,\n",
    "            dataset.num_classes,\n",
    "            aggr=copy.deepcopy(aggr),\n",
    "            aggr_kwargs=aggr_kwargs,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8298f-485d-46ab-b27c-7e712b1a0bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad3765-2e12-490f-b5fa-8c9fe891b183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97c3b1-e300-4057-a5c8-e3c17ae8336c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GML",
   "language": "python",
   "name": "gml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
